import json
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, ValidationError
from google import genai
from google.genai import types
from fastapi import HTTPException
import re

# --- Import PRODUCTION Models for Type Hinting and Input ---
from Campaign.video.storyboard_generator import Scene
from Campaign.video.video_bible_generator import VideoBible


# --- 1. Pydantic Models for Final Output (Modified for Batch Processing) ---

# This model defines the final prompt structure for a single scene (Unchanged)
class FinalVideoPromptOutput(BaseModel):
    """The structured output containing the final optimized prompt for one scene."""
    scene_id: int # Injected by Python
    duration: float # Injected by Python
    video_prompt: str # Generated by LLM

# NEW MODEL: The structure the LLM MUST return (a list of optimized prompts)
class LLMVideoPromptDraft(BaseModel):
    """The simplified model for the LLM's direct JSON output for ONE scene."""
    scene_id: int
    video_prompt: str # The optimized prompt string

class BatchPromptOutput(BaseModel):
    """The root object containing the list of all optimized scene outputs."""
    optimized_scenes: List[LLMVideoPromptDraft]


# --- 2. Modular Function: System Prompt for Gemini (Updated for Batch) ---

def get_optimizer_system_for_gemini_batch(is_short_form: bool = False) -> str:
    """Returns the system prompt for refining a draft into the final video generation prompt (Batch)."""
    vertical_rule = ""
    if is_short_form:
        vertical_rule = """
--- VERTICAL VIDEO RULES (9:16) ---
- MANDATORY: This is for Instagram Reels/YouTube Shorts. 
- You MUST rewrite the visual descriptions to be vertically composed.
- Use keywords: 'vertical photography', 'centered subject', 'portrait orientation', 'tall frame'.
- Avoid any mentions of 'wide shot', 'panorama', or 'landscape'.
"""

    return f"""
You are a world-class **Generative Video Model Prompt Optimization Engine**.
Your task is to take a draft visual prompt list, a video style guide (Video Bible), and crucial continuity notes, and synthesize them into a list of single, high-fidelity, comprehensive 'video_prompt' strings. This process must be done for ALL scenes in the batch.

{vertical_rule}

---
Your output MUST be a single JSON object that **strictly conforms** to the specified SCHEMA. Do not include any other text, reasoning, or markdown outside the JSON block.

---
CRITICAL INSTRUCTIONS:
1.  **Process ALL Scenes:** Iterate through the provided list of scene drafts.
2.  **Synthesis:** For EACH scene, combine ALL aesthetic directives from the VIDEO BIBLE (Color Palette, Lighting Style, Camera Style, Visual Style, Mood) with the specific scene content from the 'visual_prompt_draft'.
3.  **Continuity Enforcement:** The 'continuity_note_to_next_scene' must be a NON-NEGOTIABLE instruction embedded at the **END** of the 'video_prompt' string for the corresponding scene_id.
4.  **Final Polish:** The final 'video_prompt' must be a single, flowing, professional string optimized for generative models (e.g., include "4K cinematic quality").
---
SCHEMA (MUST BE FOLLOWED):
{{
  "optimized_scenes": [
    {{
      "scene_id": "integer",
      "video_prompt": "string"
    }}
  ]
}}
"""

# --- 3. Main Optimization Function (NEW BATCH LOGIC) ---

def optimize_video_prompts_batch(
    scene_data_list: List[Scene], 
    video_bible: VideoBible, 
    gemini_client: genai.Client,
) -> List[FinalVideoPromptOutput]:
    """
    Refines all scene draft prompts in a single batch API call and returns the list of final prompts.
    """
    
    if not gemini_client:
        raise HTTPException(status_code=500, detail="Gemini Client is not initialized.")
    if not scene_data_list:
        return []

    # Calculate this BEFORE getting the system prompt
    is_short_form = sum(round(s.end - s.start, 2) for s in scene_data_list) <= 30
    
    # Pass the boolean to the system prompt generator
    llm_system_prompt = get_optimizer_system_for_gemini_batch(is_short_form)
    
    # 2. Construct the full prompt payload for Gemini
    scene_drafts_for_llm = []
    for scene in scene_data_list:
        scene_drafts_for_llm.append({
            "scene_id": scene.scene_id,
            "draft_prompt": scene.visual_prompt_draft,
            "continuity_rule": scene.continuity_note_to_next_scene,
            "narration_summary": scene.narration_text # Provide narration context
        })
    # is_short_form = sum(round(s.end - s.start, 2) for s in scene_data_list) <= 30
    
     
     
    llm_user_prompt = f"""
    Optimize the final video generation prompts for the following list of scenes.
    
    --- SCENE DRAFT INPUT ---
    {json.dumps(scene_drafts_for_llm, indent=2)}
    
    --- GLOBAL AESTHETICS (VIDEO BIBLE) ---
    {video_bible.model_dump_json(indent=2)}
"""
    
    # 3. Call the Gemini API (ONE SINGLE CALL)
    try:
        response = gemini_client.models.generate_content(
            model='gemini-2.0-flash',
            contents=[llm_system_prompt, llm_user_prompt],
            config=types.GenerateContentConfig(
                response_mime_type="application/json", 
                temperature=0.2
            )
        )
        
        raw_json_text = response.text.strip()
        
        # VALIDATE and parse the simplified batch output
        batch_output = BatchPromptOutput.model_validate_json(raw_json_text)
        
        # 4. Final Python composition/injection of known data
        final_optimized_list: List[FinalVideoPromptOutput] = []
        scene_map = {scene.scene_id: scene for scene in scene_data_list}
        
        for optimized_scene in batch_output.optimized_scenes:
            scene_data = scene_map.get(optimized_scene.scene_id)
            
            if scene_data:
                duration = round(scene_data.end - scene_data.start, 2)
                final_optimized_list.append(
                    FinalVideoPromptOutput(
                        scene_id=optimized_scene.scene_id,
                        duration=duration,
                        video_prompt=optimized_scene.video_prompt
                    )
                )
        
        return final_optimized_list
        
    except ValidationError as e:
         raise ValueError(f"LLM output validation failed in batch prompt optimization: {e}. Raw output: {raw_json_text if 'raw_json_text' in locals() else 'N/A'}")
    except Exception as e:
        raise ValueError(f"Gemini API or JSON generation failed during Batch Prompt Optimization: {e}. Raw output: {raw_json_text if 'raw_json_text' in locals() else 'N/A'}")