import os
import json
import subprocess
from pathlib import Path
from typing import List, Optional
from pydantic import BaseModel
import re # Used for path sanitation
 
# --- 1. Pydantic Models for Input Data ---

class ImageMetadata(BaseModel):
    """
    Metadata saved alongside the image for the final video stitching process.
    """
    scene_id: int
    duration: float
    image_filename: str

 
# --- 3. Utility Function: Load Metadata ---

def load_metadata(metadata_file_path: Path) -> List[ImageMetadata]:
    """
    Loads the master metadata file generated by the image generation step.
    """
    if not metadata_file_path.exists():
        raise FileNotFoundError(f"Master metadata file not found at: {metadata_file_path}")
    
    with open(metadata_file_path, 'r') as f:
        data = json.load(f)
        
    validated_list = [ImageMetadata(**item) for item in data]
    return validated_list

# --- 4. Main Video Stitching Function ---

def stitch_slideshow_video_ffmpeg(
    metadata_file_path: Path,
    output_video_path: Path,
    srt_path: Path, 
    images_dir: Optional[Path] = None, 
    audio_file_path: Optional[Path] = None,
    bg_music_path: Optional[Path] = None,      # Added
    bg_music_volume: float = 0.12,            # Added
    delete_music_after: bool = False,         # Added
    video_fps: int = 24,
) -> Path:
    print(f"\nüé¨ [FFMPEG] Stitching Video: {output_video_path.name}")
    
    scene_metadata = load_metadata(metadata_file_path)
    campaign_dir = metadata_file_path.parent

    if images_dir is None:
        images_dir = campaign_dir / "images"
    
    if audio_file_path is None:
        # Fallback to standard naming convention
        audio_file_path = campaign_dir / "narration.mp3"
        
    if not audio_file_path.exists():
        raise FileNotFoundError(f"‚ùå Narration audio not found: {audio_file_path}")
    

    # --- NEW: DYNAMIC RESOLUTION LOGIC ---
    total_duration = sum(m.duration for m in scene_metadata)
    
    if total_duration <= 45:
        canvas_width, canvas_height = 1080, 1920
        is_short = True
        font_size = 8
        margin_v = 40 # Moves text up away from the bottom UI
        alignment = 2  # Bottom Center
        print(f"üì± Format: Vertical (9:16) for Shorts/Reels")
    else:
        canvas_width, canvas_height = 1920, 1080
        is_short = False
        print(f"üì∫ Format: Landscape (16:9)")
        font_size = 8
        margin_v = 40
        alignment = 2
    # -------------------------------------

    

    input_list = []
    filter_graph = []
    last_v_stream = ""
    success_count = 0 # TRACK ACTUAL INPUTS

    for  meta in  scene_metadata:
        image_path = images_dir / meta.image_filename
        if not image_path.exists():
            print(f"‚ö†Ô∏è Warning: Scene image {meta.image_filename} missing. Skipping.")
            continue
            
            
        input_list.append('-i')
        input_list.append(str(image_path)) 

        stream_name = f"v{success_count}" # Use success_count for indexing
        if is_short:
            # NO ZOOM - Just scale to fit exactly 1080x1920
            vf_logic = f"scale={canvas_width}:{canvas_height}:force_original_aspect_ratio=decrease,pad={canvas_width}:{canvas_height}:(ow-iw)/2:(oh-ih)/2:black"
        else:
            # Scale width to fill 1920, then crop height to 1080
            vf_logic = f"scale={canvas_width}:-1,crop={canvas_width}:{canvas_height}"
         
        # Build the visual processing per image
        filter_graph.append(
            f"[{success_count}:v]setpts=0/TB,"
            f"{vf_logic},"
            f"loop=loop=-1:size=2, setpts=N/({video_fps}*TB),"
            f"trim=duration={meta.duration:.3f}[{stream_name}]"
        )
        last_v_stream += f"[{stream_name}]"
        success_count += 1
    narration_idx = success_count      
    bg_music_idx = success_count + 1
    # --- SUBTITLE SETTINGS ---
    clean_srt = str(srt_path)
    
    sub_style = f"FontName=Arial,FontSize={font_size},PrimaryColour=&H00FFFFFF,OutlineColour=&H00000000,BorderStyle=3,Outline=1,Shadow=1,MarginV={margin_v},Alignment={alignment}"

    # --- CONCAT AND BURN SUBTITLES ---
    filter_graph.append(
        f"{last_v_stream}concat=n={success_count}:v=1:a=0[v_pre_sub]; " # Use success_count
        f"[v_pre_sub]subtitles='{clean_srt}':force_style='{sub_style}'[v_out]"
    )
    
    if bg_music_path and Path(bg_music_path).exists():
        # Mix tracks: BG music gets lowered volume, narration stays at 100%
        # duration=first ensures music doesn't outlast the narrator
        filter_graph.append(
            f"[{bg_music_idx}:a]volume={bg_music_volume}[bg_low]; "
            f"[{narration_idx}:a][bg_low]amix=inputs=2:duration=first:dropout_transition=2[a_out]"
        )
        audio_map = "[a_out]"
    else:
        audio_map = f"{narration_idx}:a"

    # Audio is the LAST input. Its index is equal to success_count
    # audio_input_index = success_count

    command = ['ffmpeg', '-y']
    command.extend(input_list)
    command.extend(['-i', str(audio_file_path)])
    
    if bg_music_path and Path(bg_music_path).exists():
        # -stream_loop -1 ensures music loops for the whole video duration
        command.extend(['-stream_loop', '-1', '-i', str(bg_music_path)])

    command.extend([
        '-filter_complex', " ; ".join(filter_graph),
        '-map', '[v_out]',
        '-map', audio_map,# Correctly map the audio
        '-c:v', 'libx264',
        '-pix_fmt', 'yuv420p',
        '-r', str(video_fps),
        '-b:v', '5000k', 
        '-c:a', 'aac',
        '-shortest', # END VIDEO WHEN AUDIO ENDS
        str(output_video_path)
    ])

    output_video_path.parent.mkdir(parents=True, exist_ok=True) 
    try:
        subprocess.run(command, check=True, capture_output=True, text=True)
        if delete_music_after and bg_music_path and Path(bg_music_path).exists():
            os.remove(bg_music_path)
        return output_video_path
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"FFmpeg failed: {e.stderr}")

# --- End Revised FFmpeg File ---